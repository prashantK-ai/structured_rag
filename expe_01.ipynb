{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet langchain langchain-community langchain-groq chromadb\n",
    "!pip install -upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (0.1.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain_huggingface) (0.25.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain_huggingface) (0.3.5)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain_huggingface) (3.1.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain_huggingface) (4.44.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.1.125)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pkhobragade\\desktop\\langgraph\\structured_rag\\stru_rag\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB 660.6 kB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.1/11.5 MB 919.0 kB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/11.5 MB 919.0 kB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/11.5 MB 919.0 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/11.5 MB 908.0 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.3/11.5 MB 1.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/11.5 MB 1.1 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.6/11.5 MB 1.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/11.5 MB 2.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/11.5 MB 2.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.1/11.5 MB 2.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/11.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/11.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.5/11.5 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 3.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.9/11.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.5 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.3/11.5 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/11.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.8/11.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.1/11.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.3/11.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.6/11.5 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.5 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.1/11.5 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.0/11.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.0/11.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.4/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.9/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.1/11.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.5/11.5 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.6/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.6/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.6/11.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.4/11.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.5 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 4.8 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/292.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 20.5/292.8 kB 162.5 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 41.0/292.8 kB 245.8 kB/s eta 0:00:02\n",
      "   ---------- ---------------------------- 81.9/292.8 kB 381.3 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 174.1/292.8 kB 697.2 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 204.8/292.8 kB 778.2 kB/s eta 0:00:01\n",
      "   -------------------------------------  286.7/292.8 kB 842.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 292.8/292.8 kB 784.9 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Langchain Module\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLm\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 0}, page_content='FOOD PUZZLE : Developing Large Language Model Agents as Flavor Scientists\\nTenghao Huang1, Donghee Lee2*, John Sweeney*, Jiatong Shi, Emily Steliotes2,\\nMatthew Lange2,Jonathan May1,Muhao Chen2\\n1University of Southern California\\n2University of California, Davis\\ntenghaoh@usc.edu\\nAbstract\\nFlavor development in the food industry is in-\\ncreasingly challenged by the need for rapid\\ninnovation and precise flavor profile creation.\\nTraditional flavor research methods typically\\nrely on iterative, subjective testing, which lacks\\nthe efficiency and scalability required for mod-\\nern demands. This paper presents three con-\\ntributions to address the challenges. Firstly,\\nwe define a new problem domain for scien-\\ntific agents in flavor science, conceptualized\\nas the generation of hypotheses for flavor pro-\\nfile sourcing and understanding. To facili-\\ntate research in this area, we introduce the\\nFOOD PUZZLE , a challenging benchmark con-\\nsisting of 978 food items and 1,766 flavor\\nmolecules profiles. We propose a novel Sci-\\nentific Agent approach, integrating in-context\\nlearning and retrieval augmented techniques to\\ngenerate grounded hypotheses in the domain\\nof food science. Experimental results indicate\\nthat our model significantly surpasses tradi-\\ntional methods in flavor profile prediction tasks,\\ndemonstrating its potential to transform flavor\\ndevelopment practices.\\n1 Introduction\\nThe burgeoning demand for novel and appealing\\nflavors in the food industry necessitates expedited\\ninnovation cycles without compromising on quality\\n(Hofmann et al., 2018). Traditionally, the process\\nof ingredient sourcing and evaluation for flavor de-\\nvelopment has been reliant on manual assessment\\nand human expertise. In addition to being labor-\\nintensive, and subjective, this process also relies\\non time-consuming iterations, hindering the pace\\nof flavor development (Engeseth and Ac Pangan,\\n2018; Hofmann et al., 2018; Patel and Patel, 2019).\\nAdditionally, complex flavor interactions, together\\nwith the vast array of available ingredients, present\\nformidable challenges for food scientists in opti-\\nmizing flavor profiles (Hamilton and Lahne, 2020).\\n*Equal contribution.\\nFigure 1: Flavor is determined by diverse flavor\\nmolecules. Sourcing and identifying these molecules\\nfrom various foods is a time-consuming and resource-\\nintensive task for food scientists. Understanding these\\nconnections is crucial in flavor science for developing\\nand enhancing food products to ensure appealing taste\\nexperiences for consumers. In this work, we explore\\nhow LLMs can assist in this process.\\nRecent advancements in Large Language Mod-\\nels (LLMs) have revolutionized traditional scien-\\ntific methodologies across a broad spectrum of\\ndisciplines. In fields ranging from biology and\\nchemistry to physics and material sciences, re-\\nsearchers have employed LLM technologies to an-\\nalyze complex datasets, uncover hidden patterns,\\nand narrow down the search spaces of scientific\\nproblems (Horawalavithana et al., 2022; Singhal\\net al., 2022; O’Donoghue et al., 2023; Song et al.,\\n2023; Hong et al., 2024). This trend highlights the\\ntransformative potential of LLMs in reshaping tra-\\nditional paradigms and catalyzing breakthroughs.\\nThis paper explores how the integration of LLMsarXiv:2409.12832v1  [cs.CL]  19 Sep 2024'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 1}, page_content='Figure 2: A high level overview of the F OOD PUZZLE data hierarchy\\ncan streamline and enhance the process of evaluat-\\ning ingredient sources for their flavor-giving poten-\\ntial. However, the application of LLMs in ingredi-\\nent sourcing and flavor formulation is hindered by\\na significant barrier: the absence of high-quality,\\ndomain-specific datasets. Most LLMs are trained\\non generic data from the Web, which often lacks\\nthe detailed, specialized knowledge required for ac-\\ncurate flavor profile understanding and prediction.\\nIn this paper, we delineate three primary contri-\\nbutions that collectively advance the application of\\nLLMs in the flavor science. Our firstcontribution\\nis to formulate flavor profile sourcing and under-\\nstanding as a LLM agent problem. Flavor sourcing\\nand understanding are inherently complex tasks,\\ninvolving an open-ended exploration of vast ingre-\\ndient arrays to pinpoint components that deliver de-\\nsired profiles. Scientific agents excel at navigating\\nthese complexities by leveraging their capacity to\\nidentify relevant evidence and reason within large\\ncontext spaces effectively. When integrated with\\nexternal knowledge sources, these agents can per-\\nform the labor-intensive tasks of flavor sourcing\\nand understanding with enhanced efficiency and\\nprecision.\\nOur second contribution is the creation of the\\nFOOD PUZZLE dataset. To gather data for our\\nstudy on flavor profile prediction and understand-\\ning, we turn to FlavorDB (Garg et al., 2018). Fla-\\nvorDB is a comprehensive database that includes\\ndetailed information on 25,595 flavor molecules,\\nof which 1,766 are specifically noted for their pres-\\nence in 978 natural ingredients. Figure 2 presents\\nan overview of the structure of our constructed\\ndataset. We propose two tasks that mirror real-\\nworld scientific study. The first task is Molecu-\\nlar Food Prediction (MFP), which challenges the\\nmodel to predict possible food items based on agiven set of flavor molecules. The second task is\\ncompleting the molecular profile (MPC) which re-\\nquires the identification of flavor molecules likely\\npresent in a specified food item.\\nAs we realize the challenge of existing ap-\\nproaches for solving the FoodPuzzle task, our\\nthird contribution is the development of a com-\\nprehensive scientific agent method for proposing\\ngrounded hypotheses of flavor profile prediction\\ntasks1. By integrating in-context learning (ICL)\\nand the Retrieval-Augmented Generation (RAG)\\napproach, our method enhances the model’s abil-\\nity to produce accurate and reliable results while\\nproviding traceable justifications for its predictions,\\nthus addressing key domain-specific knowledge\\nand improving reliability in flavor development\\napplications. Experimental results show that our\\nproposed scientific agent method significantly out-\\nperforms baseline methods.\\nOur work represents a pilot study to design sci-\\nentific agents for flavor profile sourcing tasks. We\\nset a new benchmark for leveraging advanced AI\\ntechnologies in the domain of flavor science. Our\\nmethodology provides clear, traceable insights into\\nhow flavor profiles are derived, thus facilitating\\ngreater acceptance and use of LLMs in practical\\nfood science applications.\\n2 Related Works\\nRetrieval-Augmented Generations. Retrieval-\\nAugmented Language Models (RALMs) enhance\\nthe reasoning process by incorporating external\\nknowledge sources, thus providing additional\\ncontextual information (Chen et al., 2017; Lee\\net al., 2019b; Karpukhin et al., 2020; Borgeaud\\net al., 2022). Early works such as Retrieval-\\n1Our codes and the dataset will be publicly released upon\\nacceptance.'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 2}, page_content='Augmented Generation (RAG; Lewis et al. 2020)\\nand Retrieval-Augmented Language Model pre-\\ntraining (REALM; Guu et al. 2020) have demon-\\nstrated significant improvements in contextual un-\\nderstanding and question-answering by fetching\\nrelevant documents from large corpora. Recent ad-\\nvancements in RALMs have significantly enhanced\\nlanguage models by integrating external knowl-\\nedge dynamically and efficiently, improving perfor-\\nmance with minimal retraining (Ram et al. 2023;\\nJiang et al. 2023; Izacard et al. 2024). In contrast\\nto conventional RAG approaches retrieving from a\\nstatic index base, our method leverages scholarly\\narticles from the internet and augments model rea-\\nsoning with consolidated information about flavor\\nscience. This approach innovates upon conven-\\ntional RAG systems and highlights the extendabil-\\nity of our Flavor Scientific Agent.\\nLLMs for Scientific Research. The application\\nof large language models (LLMs) in scientific re-\\nsearch has led to significant advancements across\\nvarious disciplines. For example, domain-specific\\nadaptations of BERT (Devlin et al., 2019) for biol-\\nogy and biomedical domains have demonstrated\\nsignificant performance improvements in tasks\\nsuch as protein classification and text mining, un-\\nderscoring the value of tailored pretraining for sci-\\nentific corpora (Lee et al. 2019a; Vig et al. 2021;\\nBeltagy et al. 2019; Gu et al. 2021; Taylor et al.\\n2022). In the field of chemistry, LLMs have been\\ntailored for tasks such as reaction prediction and\\nmolecule translation (Lu and Zhang 2022; Edwards\\net al. 2022a; Sagawa and Kojima 2023). However,\\nthey face common challenges, including a depen-\\ndency on large, high-quality scientific datasets for\\ntraining, which are often difficult to collect, and a\\nlack of interpretability in their predictions, making\\nit challenging for chemists to trust and use the re-\\nsults effectively (Horawalavithana et al. 2022; Bran\\net al. 2023). Our method leverages retrieval aug-\\nmented techniques to provide in-context learning at\\ninference time, outperforming training-time meth-\\nods. Our approach not only streamlines the process\\nby generating and selecting the best hypothesis\\nfrom multiple candidates but also enhances inter-\\npretability by providing explicit evidence of the\\nrationales behind the outputs.\\n3 Task and Data\\nThis section delineates the tasks and data designed\\nto evaluate the efficacy of LLMs in predicting\\nFigure 3: Distribution of the number of flavor molecules\\nin the FOODPUZZLE dataset.\\nfood items based on their molecular flavor profiles.\\nThese tasks are intended to assess the LLMs’ capa-\\nbilities in contextually-enhanced reasoning within\\nthe domain of molecular data.\\n3.1 Task Definition\\nMolecular Food Prediction (MFP). We define\\nthe task as learning the mapping function hwhich\\ntakes a set of molecules M={m1, m2, . . . , m n}\\nas input and outputs a corresponding food source\\nF. This can be expressed as:\\nF=h(M).\\nThis formulation aims to predict food sources based\\nsolely on their molecular composition, without re-\\nliance on specific algorithms or systems.\\nMolecular Profile Completion (MPC). The pri-\\nmary objective of the Molecular Profile Completion\\n(MPC) task is to identify the missing molecules\\nneeded to complete the molecular profile of a given\\nfood item. The function greceives the known food\\nitemF, a partial set of its molecules Mpartial , and an\\ninteger nrepresenting the expected number of miss-\\ning molecules. It then outputs the set of missing\\nmolecules Mmissing . This can be mathematically\\nrepresented as:\\nMmissing =g(F, M partial, n).\\nIn this task, the integer nis provided as part of the\\ninput, indicating the number of molecules that are\\nmissing.\\n3.2 Constructing the F OOD PUZZLE Dataset\\nData Collection. To gather data for our study on\\nflavor molecule prediction and analysis, we crawl'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 3}, page_content='Figure 4: PCA visualization illustrating clustering of\\nfood entities based on molecular profiles.\\nthrough the extensive collection of food entries of\\nthe the Flavor DB2. From the collected data, we\\ncreate three distinct stores of information. The\\nfirst store contains profile information pertaining\\nto each molecule (e.g., 2-Ethylpyrazine), detailing\\nits properties (e.g., isotope atom count) and fla-\\nvor characteristics (e.g., ordor). The second store\\nfocuses on information related to each food item,\\ncategorizing them by type and listing associated\\nmolecular compositions. Finally, we construct an\\nassociation matrix that mapped which foods were\\nassociated with which molecules, represent as pairs\\nof food and molecule identifiers. This structured\\napproach ensures that our dataset was both com-\\nprehensive and well-organized, facilitating subse-\\nquent analysis and modeling efforts. The resulting\\nFOODPUZZLE dataset maps 978 foods to 1766 fla-\\nvor molecules. For the purpose of fine-tuning on\\nthe baseline, we split the dataset into train/dev/test\\nsets by 80%/10%/10%.\\nData Analysis. Our dataset includes a variety of\\nfood entities, each associated with a unique set\\nof molecules. Figure 3 reveals that the majority\\nof food entities are associated with a relatively\\nsmall number of molecules, while a few entities\\nhave a significantly higher number of associated\\nmolecules. We employed feature encoding to rep-\\nresent each food item by its molecular composi-\\ntion, resulting in each item being characterized\\nby a 1766-dimensional vector. Principal Compo-\\nnent Analysis (PCA) of the encoded vectors, illus-\\n2The address of FlavorDB endpoint - https://cosylab.\\niiitd.edu.in/flavordbtrated in Figure 4, reveals distinct clustering for\\ncategories such as cheese, whisky, oil, and fruit.\\nThese clusters indicate that food entities with anal-\\nogous molecular compositions tend to aggregate,\\nsuggesting inherent patterns in flavor molecules\\nthat could potentially be predicted using machine\\nlearning techniques.\\n3.3 Evaluation Protocols\\nEvaluation of Molecular Food Prediction (MFP).\\nThe objective of MFP is to predict the category of\\na food item based on its molecular composition.\\nThere are 21 categories as shown in Table 1. To\\nmeasure the correctness of predicted food category,\\nwe provide the groundtruth answer F∗, which is\\nsourced from FlavorDB. At evaluation time our\\nmodels predict a food category ˆFbut this is done as\\na free text prediction; no ontology is provided. We\\nthus compare ˆFandF∗with a language model3to\\nassess whether the two are semantically equivalent.\\nIn this way we are able to match, e.g., “Alcohol”\\nand“Alcoholic Beverages” , which are semanti-\\ncally equivalent but of different formats. We report\\naccuracy as the metric for this task.\\nCategories\\nCereal Fruit Essential Oil\\nPlant Bakery Fungus\\nSeed Dish Spice\\nFlower Nut and Seed Beverage\\nAnimal Product Vegetable Plant Derivative\\nAdditive Meat\\nFish and Seafood Cereal Crop\\nDairy Herb\\nTable 1: Classification of food items into macro cate-\\ngories in the dataset.\\nEvaluation of Molecular Profile Completion\\n(MPC). MPC assesses the model’s proficiency in\\npredicting missing flavor molecules. To measure\\nthe correctness of predicted missing molecules, we\\nprovide the groundtruth answer set M∗. At eval-\\nuation time our models predict a set of molecules\\nˆM. We thus calculate the F1 score using f(ˆM)\\nandf(M∗), where fextracts the set of functional\\ngroups of the provided set of molecules. Mathe-\\nmatically, the F1 score is calculated as below:\\nF1 =2· |f(ˆM)∩f(M∗)|\\n|f(ˆM)|+|f(M∗)|.\\n3In this work we use GPT-3.5-turbo; prompt details are\\nprovided in the Appendix B.'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 4}, page_content='By focusing on functional groups, this evaluation\\nprotocol leverages the fact that chemicals sharing\\nfunctional groups tend to exhibit similar properties\\n(Garg et al., 2017). In this way we are able to pri-\\noritize chemical functionality over strict structural\\nsimilarity, which ensures predictive relevance and\\nstreamline the evaluation process.\\n4 Evaluated Methods\\nIn this section, we explore selected baseline models\\n(§4.1) and delve into the implementation details of\\nour scientific agent designed for flavor development\\n(§4.2).\\n4.1 Baseline Methods\\nWe evaluate the following baseline models to es-\\ntablish benchmarks for assessing the capabilities\\nof our proposed scientific agent in the domain of\\nflavor development. Our evaluation is organized\\ninto three distinct categories of models.\\nFoundation LLMs. We evaluate ChatGPT-3.5-\\nturbo4, Gemini-1.5-Pro, and Llama-3-instruct-8B\\n(Touvron et al., 2023) using zero-shot inference.\\nThough being general-purpose and not specifically\\ntailored to food science, these models are renowned\\nfor their robust performance across a broad spec-\\ntrum of natural language processing tasks. The\\nprompts used for zero-shot inference were care-\\nfully designed to ensure clarity and relevance to the\\ntasks.\\nDomain-specific LLMs. We recognize that\\nmolecule names and related chemical knowledge\\ncan be unfamiliar to general-purpose LLMs. Hence,\\nwe also consider models trained on corpora from\\nrelated domains as baselines. These include MolT5,\\na model pretrained on a vast amount of unlabeled\\nnatural language text and molecule strings (Ed-\\nwards et al., 2022b), and BioT5, which is pretrained\\non structured and unstructured biology corpora\\nfor generative tasks involving molecular entities\\n(Pei et al., 2023). Since these two models are not\\ninstruction-tuned for conversational interaction, we\\nfollow Wang et al. (2022) to constrain the vocabu-\\nlary set at decoding time and fine-tune these models\\non the training set for the task.\\nBoth BioT5 and MolT5 are fine-tuned on our\\ntrain set. We use 4 Nvidia A10G GPUs for our\\nexperiments. The models were fine-tuned with a\\nbatch size of 16 for the MFP task and 8 for the MPC\\n4April 29th 2024 version.task, using the AdamW optimizer with a learning\\nrate of 0.00005, β1= 0.9, and β2= 0.999. Each\\nmodel was trained for 20 epochs, with early stop-\\nping applied after 3 epochs based on validation\\nloss.\\nIn-context Learning. We also consider in-context\\nlearning techniques for our tasks. However,\\nmolecule names may be out-of-context informa-\\ntion for LLMs due to limited exposure during pre-\\ntraining, which can impair the performance of the\\nagents. To enhance reasoning capabilities, we em-\\nploy a retriever to selectively identify relevant data\\npoints from the labeled training set, providing sup-\\nplementary information. For the implementation\\nof this retriever, we evaluate several approaches:\\nBM25 (Robertson and Walker, 1994), Sentence\\nTransformer (Reimers and Gurevych, 2019), and\\nDense Passage Retrieval (DPR; Karpukhin et al.\\n2020). Our empirical results indicate that BM25\\nsignificantly outperforms dense retrieval methods,\\nachieving a 23.2% accuracy, compared to 13.1%\\nfor the Sentence Transformer and 18.0% for DPR\\nin end-to-end performances on the MFC task. This\\nsuperior performance of BM25 aligns with our\\nexpectations, given that dense retrievers are pre-\\ntrained and fine-tuned on generic natural language\\ncorpora, which may not effectively generalize to\\nretrieve domain-specific data.\\nFor both tasks, the BM25 retriever retrieves the\\ntopkmost similar demonstrations to the query\\nfrom a dense corpus index, where each passage is\\nstructured in the format: “Food: F′. Molecules:\\nM′”. These retrieved demonstrations are then\\npresented to the model as demonstrations, which\\nare crucial for generating grounded and evidence-\\nbased output.\\n4.2 Architecture of the Scientific Agent\\nWe propose an advanced hybrid methodology that\\nintegrates Retrieval-Augmented Generation (RAG)\\nwith online scholarly sources and in-context learn-\\ning, utilizing pertinent demonstrations. This ap-\\nproach is specifically tailored to surmount the chal-\\nlenges inherent in general-purpose LLMs that lack\\nspecialized knowledge in the domain of food sci-\\nence. The architecture of our scientific agent is de-\\nsigned to mirror the investigative processes typical\\nof human scientists. This alignment not only en-\\nhances the agent’s ability to generate scientifically\\nrobust hypotheses but also improves the explicabil-\\nity of errors, thereby contributing to its operational'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 5}, page_content='Figure 5: Architecture of the proposed Scientific Agent\\ntransparency and efficacy in flavor science tasks.\\nStarting Point Identification. Similarly to solving\\nother puzzles, it is important to identify suitable\\nstarting points in our flavor profile sourcing task.\\nConsequently, for the MFP, given a list of flavor\\nmolecules, denoted as M, we focus on selecting\\nmolecules that are pivotal for our analysis. We cal-\\nculate the information entropy for each molecule\\ninMbased on the frequency of appearances in the\\ntrain set and select up to 10 molecules with the\\nlowest information entropy to identify an informa-\\ntive subset of molecules. These selected molecules\\nM∗then serve as starting points for subsequent\\ninvestigations.\\nInformation Collection. One key challenge in em-\\nploying general-purpose LLMs in specialized fields\\nlike food science is their inherent limitation in do-\\nmain specificity. While these models are trained\\non massive corpora and varied task datasets that\\nprovide a wide range of general knowledge, they\\noften lack the detailed, specific insights needed\\nto tackle complex, domain-focused issues effec-\\ntively. This shortfall is particularly noticeable in\\nfood science, where the models struggle to accu-\\nrately comprehend or predict the subtle nuances of\\nflavor profiles and the interactions among differ-\\nent compounds. Though in-context learning intro-\\nduces relevant demonstrations to the agent, it still\\npredominantly relies on the parametric knowledge\\nof LLMs to figure out the relationship between\\ndemonstrations and test instances. This inherent\\nlimitation can result in outputs that may not suffi-\\nciently use in-context information.\\nTo address this gap, Out-of-Domain Evidence is\\nsourced from scholarly articles and internet blogs in\\naddition to the in-context demonstrations. Specif-\\nically, for the MFP task, the process begins by\\niteratively querying each molecule m∈M∗. Thequery is formulated as: “What are the common\\nfood sources that could contain m?”Similarly,\\nfor the MPC task, the process begins by querying\\neach food item F, with the corresponding query:\\n“What molecules are present in F?”We then use the\\nGoogle Custom Search API to identify academic\\narticles that detail which plants, animals, or other\\norganisms naturally produce these molecules. To\\nensure the reliability of the information retrieved,\\nthe search is confined to scientific repositories such\\nas PubMed5and arXiv6. The scientific agent gath-\\ners the sourced articles, storing the information as\\nevidence. To enhance efficiency during inference,\\nthe Google Custom Search is executed offline, and\\nthe results are stored locally. Consequently, during\\ninference, the system simply retrieves the relevant\\ninformation from local storage.\\nHypothesis Generation. The final phase is hy-\\npothesis generation. We realize that it is possible\\nfor the collected evidences to sometimes contradict\\neach other, pointing to different answers. Leverag-\\ning the Chain-of-Thought (CoT) approach (Wei\\net al., 2022), we adopt a role-play based agent\\nframework to process sourced evidences and gener-\\nate the final hypothesis. Specifically, we initialize\\ntwo role-based models: the Scientist that proposes\\nthree hypotheses and the Reviewer that evaluates\\nthe argument quality and selects the best hypothe-\\nsis7. In the MFP task, we retrieve locally stored\\nevidence from FlavorDB related to up to ten key\\nmolecules identified during the initial point identi-\\nfication phase. This evidence, along with relevant\\ndata points, is then provided to the Scientist model.\\nFor the MPC task, as there is no starting point iden-\\ntification process, we retrieve locally evidence re-\\n5https://pubmed.ncbi.nlm.nih.gov\\n6https://arxiv.org\\n7We include prompt examples in Appendix B'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 6}, page_content='Category Model MFP accuracy(%) MPC F1 score\\nDomain-Specific LLMsMolT5 (Edwards et al., 2022b) 9.8 0.144\\nBio-T5 (Pei et al., 2023) 16.6 0.278\\nZero-shotLLaMA3-8B-instruct 15.0 0.292\\nGemini1.5 Pro 19.0 0.340\\nGPT-3.5 Turbo 12.2 0.327\\nIn-context LearningLLaMA3-8B-instruct 31.6 0.349\\nGemini1.5 Pro 34.6 0.373\\nGPT-3.5 Turbo 23.2 0.360\\nScientist Agent LLaMA3-8B-instruct 35.5 0.374\\nGemini1.5 Pro 34.2 0.333\\nGPT-3.5 Turbo 26.9 0.374\\nTable 2: Combined Results for Model Accuracies in Molecule Food Prediction and F1 Scores for Molecule Profile\\nCompletion\\nlated to the food item directly from FlavorDB, and\\nthis evidence is provided to the Scientist model.\\nAlso, as implemented in the in-context learning\\nbaseline, a BM25 retriever is used to select the\\nthree most similar demonstrations, which are also\\ngiven to the Scientist model. The Scientist, there-\\nfore, receives data input from FOOD PUZZLE , the\\nretrieved evidence, and demonstrations, and uses\\nthese to generate three hypotheses. The Reviewer\\nmodel then receives the data input, samples, and\\nthe three hypotheses generated by the Scientist. Af-\\nter reviewing the hypotheses, the Reviewer might\\nreject or select the most suitable one. Finally, the\\nscientific model returns this best hypothesis as the\\nfinal prediction.\\n5 Experiments\\nIn this section, we report and analyze the bench-\\nmarking results ( §5.1). We randomly select 50\\nprediction errors and manually inspect them. We\\npresent a comprehensive error analysis that outlines\\nkey categories of errors as insights for future model\\nrefinement ( §5.2).\\n5.1 Main Results\\nTable 2 presents the combined results for model\\naccuracies in Molecule Food Prediction (MFP) and\\nF1 scores for Molecule Profile Completion (MPC).\\nDomain-Specific LLMs such as MolT5 and Bio-T5\\nexhibit lower performance in both MFP accuracy\\nand MPC F1 score compared to other models. The\\nlimitation of these models can be attributed to two\\nreasons. On the one hand, their specialization inmolecular and biological text does not necessarily\\ntranslate to broader applications in food science.\\nHowever, small parameter sizes and lack of instruc-\\ntion tuning might also limit model performance.\\nIn the zero-shot scenario, foundation LLMs like\\nLLAMA3-8B-Instruct, Gemini1.5 Pro, and GPT-\\n3.5 Turbo show varying degrees of effectiveness.\\nLLAMA3-8B-Instruct achieves an MFP accuracy\\nof 15.0% and an F1 score of 0.292, indicating mod-\\nerate performance. Gemini1.5 Pro performs better\\nwith an MFP accuracy of 19.0% and an F1 score\\nof 0.340, demonstrating its relative strength in both\\nfood prediction and molecule profile completion.\\nGPT-3.5 Turbo balances its performance with an\\nMFP accuracy of 12.2% and an F1 score of 0.327.\\nThese results highlight the challenging nature of\\nour tasks and suggest that while general-purpose\\nmodels can achieve reasonable performance, they\\nstill lack the domain-specific knowledge required\\nfor food science tasks.\\nLeveraging in-context learning techniques,\\nLLMs demonstrate notable improvements over the\\nfoundation and domain-specific LLMs. The en-\\nhanced performance of these models underscores\\nthe importance of in-context learning in provid-\\ning relevant additional information and improving\\nprediction accuracy. Furthermore, these results\\nhighlight the effectiveness of our FOOD PUZZLE\\ndataset in enabling models to take advantage of the\\nrelevant context for better predictions.\\nThe Autonomous Scientist Agent exhibits supe-\\nrior performance in both Molecular Food Predic-\\ntion accuracy and F1 scores for Molecular Profile'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 7}, page_content='Completion, compared to other model categories.\\nThe LLaMA3-8b-instruct model within the agent\\nachieves an MFP accuracy of 35.5%, outperform-\\ning the best in-context learning model (Gemini1.5\\nPro with 34.6%) and the highest foundation LLM\\n(Gemini1.5 Pro with 19.0%). Furthermore, the\\nagent shows robust F1 scores, with LLaMA3-8b-\\ninstruct at 0.374, Gemini1.5 Pro at 0.333, and GPT-\\n3.5 Turbo at 0.374, indicating its balanced strength\\nin both tasks. This approach effectively addresses\\nthe challenges faced by general-purpose LLMs that\\nlack domain-specific knowledge in food science.\\nBy sourcing and consolidating domain-specific evi-\\ndence and employing a structured approach to hy-\\npothesis generation, the agent enhances prediction\\naccuracy and molecule profile completion.\\n5.2 Error Analysis\\nNext, we define a taxonomy of error types dis-\\ncerned among evaluated scientific agents, based on\\nan analysis of fifty randomly selected prediction\\nerrors. It is possible that a single example may\\nexhibit multiple error types concurrently. To pre-\\nclude redundancy, only the most salient error per\\nexample is accounted for in this analysis.\\nInappropriate Initialization of Search Space\\n(32%). When identifying key molecules to un-\\nderstand flavor profiles, the autonomous scientific\\nagent often struggle due to limited domain-specific\\nknowledge. For example, while domain experts\\ncan focus on molecules such as “hydrogen sulfide”\\nand “diethyl sulfide” for their significant roles in\\nfoods like eggs, LLMs tend to suggest more generic\\nmolecules such as thiamine and betaine. These\\nare common across various foods and can lead the\\nagent to inaccurate, fruit-biased interpretations by\\nnarrowing the scope of analysis. This mismatch\\nhighlights the importance of incorporating specific\\nand expert-driven insights into the training and in-\\nference of LLMs used in flavor science.\\nEpistemic Hallucination (26%). The phe-\\nnomenon of hallucination was observed across the\\nevaluated LLMs. During processes intended to en-\\nhance model reasoning through chain-of-thought\\ntechniques, the output generated by the agent often\\nlacks grounding in factual evidence. For example,\\nwhen asked to identify flavor molecules for eggs,\\nan LLM might list molecules commonly found in\\neggs rather than those that could recreate the egg\\nflavor profile. This distinction is crucial: The goal\\nis to identify molecules that can mimic or repli-cate flavors, not just those that occur naturally in\\nthe food item. Effective prompting can mitigate\\nthis issue. Clearly specifying that the task involves\\nidentifying molecules capable of replicating spe-\\ncific flavors, rather than listing inherent molecules,\\ncan direct the LLMs to focus correctly. Addition-\\nally, providing explicit examples and context in the\\nprompts can further align the LLM responses with\\nthe intended task.\\nWrong Interpretation of Online Sources (20%).\\nAnother considerable source arises from the mis-\\ninterpretation of scholarly articles during the pro-\\ncess of sourcing evidence online. When models\\nasked to perform aspect-summarization toward in-\\nput queries, they exhibits a tendency to “force” an-\\nswers where direct evidence might be lacking or\\nambiguous. This compulsion to generate conclu-\\nsions can lead to erroneous or overly speculative\\nassertions. For instance, consider a scenario where\\na scholarly article states that ethanethiol is charac-\\nterized by more roasted and toasted notes, and may\\nexhibit a coffee-like character. LLMs, in its attempt\\nto generate actionable insights, might erroneously\\ninfer and assert that “coffee contains Ethanethiol\\nas one of its flavor molecules.” Such a statement is\\na speculative leap rather than a factually supported\\nconclusion, reflecting the model’s predisposition\\ntowards generating responses even in the absence\\nof clear evidence.\\n6 Future Opportunities\\nIntegrating autonomous flavor scientists and FOOD-\\nPUZZLE into the R&D pipeline offers significant\\nadvancements in flavor science, sensory science,\\nand food product development. In this section we\\ndiscuss how collaboration with wet lab and sensory\\nscientists to evaluate chemical, biological, and sen-\\nsory properties of flavor compounds will enhance\\nmodel accuracy and impact.\\nChemical Synthesis and Analysis. Using analyti-\\ncal instruments like LC-MS and GC-MS to detect\\nand quantify flavor molecules in food samples al-\\nlows for verification of structure and purity, vali-\\ndating flavor profile hypotheses. Synthetic chem-\\nistry labs can synthesize predicted candidate flavor\\nmolecules, enabling verification of their organolep-\\ntic properties. Sensory labs can manage human\\nsensory panels to evaluate synthesized flavors, fine-\\ntuning AI model predictions to align with human\\nperceptions of flavor quality and intensity.'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 8}, page_content='Biological Analysis. Biological laboratories can\\nperform bioassays and in vitro testing to assess\\nthe safety and efficacy of flavor compounds. High-\\nthroughput screening (HTS) techniques can rapidly\\ntest large libraries of flavor molecules, generating\\nextensive datasets to enhance AI predictions.\\n7 Conclusion\\nThis paper underscores the transformative poten-\\ntial of LLM agents in flavor science, particularly\\nthrough the development of an scientific agent tai-\\nlored for flavor profile sourcing. Key contributions\\ninclude the formulation of flavor profile sourcing\\nand understanding as a LLM agents task, the cre-\\nation of the FOODPUZZLE dataset from FlavorDB,\\nand the creation of a comprehensive scientific agent\\npipeline that can perform the labor-intensive tasks\\nof flavor sourcing and understanding with enhanced\\nefficiency and precision. Our findings demonstrate\\nthat our method outperforms traditional models and\\nprovides traceable and reliable predictions. This\\nwork not only sets a new benchmark for the appli-\\ncation of AI in flavor science but also paves the\\nway for further technological advancements.\\nReferences\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nERT: A pretrained language model for scientific text.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 3615–\\n3620, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, Diego\\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\\nElsen, and Laurent Sifre. 2022. Improving language\\nmodels by retrieving from trillions of tokens. In\\nProceedings of the 39th International Conference\\non Machine Learning , volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nAndres M Bran, Sam Cox, Oliver Schilter, Carlo Baldas-\\nsari, Andrew White, and Philippe Schwaller. 2023.\\nAugmenting large language models with chemistry\\ntools. In NeurIPS 2023 AI for Science Workshop .Danqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers) , pages 1870–1879,\\nVancouver, Canada. Association for Computational\\nLinguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nCarl Edwards, Tuan Lai, Kevin Ros, Garrett Honke,\\nKyunghyun Cho, and Heng Ji. 2022a. Translation\\nbetween molecules and natural language. In Proceed-\\nings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing , pages 375–413, Abu\\nDhabi, United Arab Emirates. Association for Com-\\nputational Linguistics.\\nCarl Edwards, Tuan Lai, Kevin Ros, Garrett Honke,\\nKyunghyun Cho, and Heng Ji. 2022b. Translation\\nbetween molecules and natural language. Preprint ,\\narXiv:2204.11817.\\nNicki J Engeseth and Marlon Fernando Ac Pangan.\\n2018. Current context on chocolate flavor develop-\\nment — a review. Current Opinion in Food Science ,\\n21:84–91.\\nNeelansh Garg, Apuroop Sethupathy, Rudraksh Tuwani,\\nRakhi NK, Shubham Dokania, Arvind Iyer, Ayushi\\nGupta, Shubhra Agrawal, Navjot Singh, Shubham\\nShukla, Kriti Kathuria, Rahul Badhwar, Rakesh\\nKanji, Anupam Jain, Avneet Kaur, Rashmi Nagpal,\\nand Ganesh Bagler. 2017. FlavorDB: a database\\nof flavor molecules. Nucleic Acids Research ,\\n46(D1):D1210–D1216.\\nNeelansh Garg, Apuroop Sethupathy, Rudraksh Tuwani,\\nRakhi Nk, Shubham Dokania, Arvind Iyer, Ayushi\\nGupta, Shubhra Agrawal, Navjot Singh, Shubham\\nShukla, et al. 2018. Flavordb: a database of flavor\\nmolecules. Nucleic acids research , 46(D1):D1210–\\nD1216.\\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\\nGao, and Hoifung Poon. 2021. Domain-specific lan-\\nguage model pretraining for biomedical natural lan-\\nguage processing. ACM Trans. Comput. Healthcare ,\\n3(1).\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\\nand Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In Proceedings of the\\n37th International Conference on Machine Learning ,\\nvolume 119 of Proceedings of Machine Learning\\nResearch , pages 3929–3938. PMLR.'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 9}, page_content='Leah M Hamilton and Jacob Lahne. 2020. Fast and auto-\\nmated sensory analysis: Using natural language pro-\\ncessing for descriptive lexicon development. Food\\nQual. Prefer. , 83:103926.\\nThomas Hofmann, Dietmar Krautwurst, and Peter\\nSchieberle. 2018. Current status and future perspec-\\ntives in flavor research: Highlights of the 11th wart-\\nburg symposium on flavor chemistry & biology. J.\\nAgric. Food Chem. , 66(10):2197–2203.\\nSirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu,\\nDanyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang,\\nLingyao Zhang, Mingchen Zhuge, Taicheng Guo,\\nTuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang,\\nXiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng\\nCheng, Zongze Xu, Chenglin Wu, Li Zhang, Min\\nYang, and Xiawu Zheng. 2024. Data interpreter: An\\nllm agent for data science. ArXiv , abs/2402.18679.\\nSameera Horawalavithana, Ellyn Ayton, Shivam\\nSharma, Scott Howland, Megha Subramanian, Scott\\nVasquez, Robin Cosbey, Maria Glenski, and Svit-\\nlana V olkova. 2022. Foundation models of scientific\\nknowledge for chemistry: Opportunities, challenges\\nand lessons learned. In Proceedings of BigScience\\nEpisode #5 – Workshop on Challenges & Perspec-\\ntives in Creating Large Language Models , pages 160–\\n172, virtual+Dublin. Association for Computational\\nLinguistics.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\\nYu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2024. Atlas: few-shot learning with retrieval\\naugmented language models. J. Mach. Learn. Res. ,\\n24(1).\\nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun,\\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\\nCallan, and Graham Neubig. 2023. Active retrieval\\naugmented generation. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 7969–7992, Singapore. As-\\nsociation for Computational Linguistics.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for open-\\ndomain question answering. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 6769–6781,\\nOnline. Association for Computational Linguistics.\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\\n2019a. BioBERT: a pre-trained biomedical language\\nrepresentation model for biomedical text mining.\\nBioinformatics , 36(4):1234–1240.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019b. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Computa-\\ntional Linguistics , pages 6086–6096, Florence, Italy.\\nAssociation for Computational Linguistics.Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems , volume 33, pages 9459–\\n9474. Curran Associates, Inc.\\nJieyu Lu and Yingkai Zhang. 2022. Unified deep learn-\\ning model for multitask reaction predictions with\\nexplanation. Journal of Chemical Information and\\nModeling .\\nOdhran O’Donoghue, Aleksandar Shtedritski, John Gin-\\nger, Ralph Abboud, Ali Ghareeb, and Samuel Ro-\\ndriques. 2023. Bioplanner: Automatic evaluation of\\nllms on protocol planning in biology. In Proceed-\\nings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , pages 2676–2694.\\nJayvadan Patel and Anita Patel. 2019. Flavor manufac-\\nturing and selection criteria for functional food and\\nnutraceuticals industries. In Flavor Development for\\nFunctional Foods and Nutraceuticals , pages 39–72.\\nCRC Press.\\nQizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan\\nGao, Lijun Wu, Yingce Xia, and Rui Yan. 2023.\\nBioT5: Enriching cross-modal integration in biol-\\nogy with chemical knowledge and natural language\\nassociations. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 1102–1123, Singapore. Association for\\nComputational Linguistics.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. Transactions of the Association for\\nComputational Linguistics , 11:1316–1331.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\\nSentence embeddings using siamese bert-networks.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing . Associa-\\ntion for Computational Linguistics.\\nS. E. Robertson and S. Walker. 1994. Some simple\\neffective approximations to the 2-poisson model for\\nprobabilistic weighted retrieval. In Proceedings of\\nthe 17th Annual International ACM SIGIR Confer-\\nence on Research and Development in Information\\nRetrieval , SIGIR ’94, page 232–241, Berlin, Heidel-\\nberg. Springer-Verlag.\\nTatsuya Sagawa and Ryosuke Kojima. 2023. Re-\\nactiont5: a large-scale pre-trained model towards\\napplication of limited reaction data. Preprint ,\\narXiv:2311.06708.\\nK. Singhal, Shekoofeh Azizi, Tao Tu, Said Mah-\\ndavi, Jason Wei, Hyung Won Chung, Nathan\\nScales, Ajay Kumar Tanwani, Heather J. Cole-\\nLewis, Stephen J. Pfohl, P A Payne, Martin G.\\nSeneviratne, Paul Gamble, Chris Kelly, Nathaneal'),\n",
       " Document(metadata={'source': 'data\\\\FOODPUZZLE.pdf', 'page': 10}, page_content='Scharli, Aakanksha Chowdhery, P. A. Mansfield,\\nBlaise Agera y Arcas, Dale R. Webster, Greg S. Cor-\\nrado, Yossi Matias, Katherine Hui-Ling Chou, Juraj\\nGottweis, Nenad Tomaev, Yun Liu, Alvin Rajkomar,\\nJolle K. Barral, Christopher Semturs, Alan Karthike-\\nsalingam, and Vivek Natarajan. 2022. Large lan-\\nguage models encode clinical knowledge. Nature ,\\n620:172 – 180.\\nYu Song, Santiago Miret, and Bang Liu. 2023. MatSci-\\nNLP: Evaluating scientific language models on ma-\\nterials science language tasks using text-to-schema\\nmodeling. In Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 3621–3639, Toronto,\\nCanada. Association for Computational Linguistics.\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\\nScialom, Anthony Hartshorn, Elvis Saravia, An-\\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\\n2022. Galactica: A large language model for science.\\nPreprint , arXiv:2211.09085.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models. Preprint , arXiv:2307.09288.\\nJesse Vig, Ali Madani, Lav R. Varshney, Caiming\\nXiong, richard socher, and Nazneen Rajani. 2021.\\n{BERT}ology meets biology: Interpreting attention\\nin protein language models. In International Confer-\\nence on Learning Representations .\\nFei Wang, Zhewei Xu, Pedro Szekely, and Muhao Chen.\\n2022. Robust (controlled) table-to-text generation\\nwith structure-aware equivariance learning. In Pro-\\nceedings of the 2022 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5037–5048, Seattle, United States. Association for\\nComputational Linguistics.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems ,\\nvolume 35, pages 24824–24837. Curran Associates,\\nInc.'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 0}, page_content='Vision Language Models Can Parse Floor Plan Maps\\nDavid DeFazio∗, Hrudayangam Mehta∗, Jeremy Blackburn, Shiqi Zhang\\nAbstract — Vision language models (VLMs) can simultane-\\nously reason about images and texts to tackle many tasks,\\nfrom visual question answering to image captioning. This paper\\nfocuses on map parsing, a novel task that is unexplored within\\nthe VLM context and particularly useful to mobile robots.\\nMap parsing requires understanding not only the labels but\\nalso the geometric configurations of a map, i.e., what areas are\\nlike and how they are connected. To evaluate the performance\\nof VLMs on map parsing, we prompt VLMs with floorplan\\nmaps to generate task plans for complex indoor navigation.\\nOur results demonstrate the remarkable capability of VLMs\\nin map parsing, with a success rate of 0.96 in tasks requiring\\na sequence of nine navigation actions, e.g., approaching and\\ngoing through doors. Other than intuitive observations, e.g.,\\nVLMs do better in smaller maps and simpler navigation tasks,\\nthere was a very interesting observation that its performance\\ndrops in large open areas. We provide practical suggestions\\nto address such challenges as validated by our experimental\\nresults. Webpage: https://shorturl.at/OUkEY\\nI. I NTRODUCTION\\nA key to mobile robotics is a deep understanding of the\\ngeometric configuration of the world that mobile robots live\\nin. As a result, many mobile robots need some forms of a\\nmap for localization, obstacle avoidance and navigation. To\\nbuild such maps, the robots use a predefined data structure,\\ne.g., an occupancy grid [1] or visual features [2], and then\\nperform simultaneous localization and mapping (SLAM).\\nHuman beings have a long history of building and using\\nmaps. These days one can easily read floorplan maps of\\nairports and shopping centers, and figure out a plan for\\nnavigation. By comparison, robots can hardly reach com-\\nparable competency in map reading and task planning. It\\nis a non-trivial task for the robots due to the many labels\\nin the map, their ambiguous associations to different areas,\\nand complex geometric configurations. As a result, there is\\nno existing method for addressing the map parsing problem,\\ni.e., computing a navigation plan given a map image and a\\ngoal text, which motivated this research.\\nFor complex navigation tasks, a robot needs to compute\\na task plan, i.e., a sequence of navigation actions, and\\ncontinuous trajectories for realizing those actions. Example\\nactions can be entering an area andgoing through a door .\\nExtensive engineering efforts are needed to realize such\\nnavigation systems, from building the map itself to labeling\\nareas of the map. At the same time, professional architectural\\ndrafters have generated blueprints that accurately reflect\\nthe geometric configurations, which unfortunately cannot be\\nused by current robots. From an application perspective, this\\n*Equal contribution\\nAll authors are with School of Computing, Binghamton University.\\n{ddefazi1; hmehta; jblackbu; zhangs }@binghamton.edu\\nFig. 1: Quadruped robot executing a VLM-generated plan\\n(current action highlighted in red) to complete a navigation\\ntask while localizing directly on a floor plan image.\\nresearch aims to leverage the readily accessible floorplan\\nmaps in human environments to fulfill the robots’ need of\\nmaps for navigation.\\nVision language models (VLMs) are foundation models\\nthat learn from and reason about both images and texts, sup-\\nporting a variety of downstream tasks from visual question\\nanswering to image captioning. VLMs have demonstrated\\nimpressive successes in a variety of applications [3], [4], [5]\\nincluding those in robotics [6], [7]. We, for the first time,\\napply VLMs to the novel task of map parsing and evaluate its\\nperformance in navigation, a foundational problem in mobile\\nrobotics. Our approach is simple and intuitive. A floorplan\\nmap and a problem description, including the start and goal\\npositions, are provided to a VLM, and the task is to compute\\na plan (i.e., an action sequence) to achieve the goal. Despite\\nthe straightforward idea, the results are surprisingly good.\\nNavigation plans generated by VLMs which can require a\\nsequence of nine actions are generated correctly up to 90%\\nof the time on some floorplans.\\nThe main contribution of this research includes the intro-\\nduction of the map parsing problem, evaluations of VLMs\\non this problem, practical suggestions that paves the way for\\nfuture research on VLM-based map parsing, and a complete\\ndemonstration of real-robot system.\\nThere are limitations in this research that can be addressed\\nin future work. One is that we still need to slightly edit\\nthe map image, such as thickening walls and removing\\narchitectural annotations, to produce the best performance.\\nSuch steps can be automated in future work. Another is\\nthat the robot needs to stand close and forward-facing\\nwhen capturing the map image. This can be a challenge\\nto small robots because most floorplan maps are placed at\\nhuman heights. In this paper, we focus on highlighting the\\nremarkable performance of VLMs on map parsing, and leave\\nthose topics to future work.arXiv:2409.12842v1  [cs.RO]  19 Sep 2024'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 1}, page_content='Vision language model (VLM) Map \\nenhancement \\nLocalize   ApproachDoor(D1) \\n  OpenDoor(D1) \\n  GoThrough(D1) \\n  ApproachDoor(D2) \\n…\\n  GoThrough(D3) \\n“Generate a \\nnavigation plan from \\nroom N09 to room \\nT01”\\nRaw photo \\nImage prompt \\nText prompt \\nTask plan Action execution \\non a robot \\nFig. 2: Overview of our method. A robot takes a raw image of a floor plan, which is then enhanced with labels and door\\nindicators. The enhanced floor plan, along with a text prompt specifying the start and goal locations is given to a VLM.\\nThe VLM generates a navigation plan to reach the goal location, and the plan is executed on a mobile robot.\\nII. R ELATED WORK\\nIn this section, we discuss existing work in autonomous\\nnavigation for mobile robots, VLM prompting strategies, and\\nintegrating large pre-trained models in robotics. We highlight\\nhow our work differs from existing works in each of these\\ncategories.\\nA. Existing Map Representations and Navigation\\nExisting works demonstrating autonomous navigation on\\nmobile robots usually require generating an occupancy\\ngrid [8], [9], [10], [11], or leveraging vision-based meth-\\nods for simultaneous localization and mapping (Visual\\nSLAM) [2], [12], [13], [14], [15]. While such map repre-\\nsentations have proven to be effective for autonomous navi-\\ngation tasks, generating an accurate map is oftentimes labor-\\nintensive. For instance, in vision-based settings, the robot\\nhas limited knowledge of the global environment, either\\nleading to lengthy exploration, or navigational commands\\nfrom a human. In this work, we greatly reduce the effort\\nof generating accurate maps while still leveraging detailed\\ninformation of the environment through the use of existing,\\nand potentially in situ, floor plans.\\nB. VLM Prompting Strategies\\nThe output of a large pre-trained model largely relies on\\nthe way it is prompted. Strategies like chain-of-thought [16]\\nand in-context learning [17] are leveraged on LLMs to\\nimprove performance. Similar to language prompts, image\\nprompting strategies can improve VLM outputs. Set-of Mark\\nprompting, which segments and labels objects in an image\\nhas shown to improve VLM responses [18]. In our work,\\nwe design a new visual prompting strategy designed for\\nobtaining a spatial understanding of floor plan images.C. Large Models in Robotics\\nTo improve the common-sense reasoning capabilities of\\nrobots, large pre-trained LLMs and VLMs have been inte-\\ngrated in robots for various tasks like housekeeping [19], ob-\\nject rearangement [20], navigation [21], [22], [23], [24], [25],\\nand quadruped locomotion [26], [27] to name a few. Align-\\nment of the large model with the environment and robot’s\\nskills is critical to perform tasks in the real world [28]. Vari-\\nous approaches have demonstrated planning capabilities [29],\\n[30] and uncertainty estimation [31] of large models. Various\\nvisual prompting strategies designed for different robotic\\nmanipulation tasks have also been developed [6], [7]. In line\\nwith recent works that leverage large models to incorporate\\ncommon-sense knowledge on robots, we generate feasible\\nnavigation plans directly from an image of a floor plan.\\nIII. M ETHODOLOGY\\nIn this section, we present our approach for leveraging\\nVision-Language Models (VLMs) to interpret floor plan\\nimages and generate navigation instructions. We discuss the\\ntwo key aspects of our approach: visual prompting strategy,\\nand VLM-based plan generation.\\nA. Visual Prompting Strategy\\nOur study uses floor plan images that include detailed\\narchitectural layouts for various building types. To generate\\naccurate plans from a VLM, it’s important to design a visual\\nprompt which can facilitate learning the structure of the\\nfloor plan. Unfortunately, raw floor plan images tend to\\ncontain various markings (i.e. windows, furniture symbols,\\nnon-uniform wall thickness) which can potentially confuse\\nthe VLM in understanding the general layout of the floor'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 2}, page_content=\"plan. Thus, we remove such markings to produce a cleaner\\nmap which can be better leveraged by a VLM.\\nWe find that removing extraneous details from the floor\\nplan is insufficient for the VLM to understand the map\\nlayout. In particular, we find the VLM has limited spatial\\nawareness for sparsely labelled rooms with lots of open\\nspace, and near key decision points like doors and inter-\\nsections. To alleviate this, we add duplicate room labels in\\nopen spaces and near doors and intersections. This provides\\nthe VLM with further guidance in understanding the gen-\\neral structure of the floor plan. We later demonstrate the\\nimportance of such additional labels in Section IV. In this\\nwork, we manually remove extraneous markings and add\\nadditional room labels for visual prompt construction. We\\nleave automated floor plan editing as future work.\\nMap enhancement with dense labeling: A key finding\\nin this research is the importance of dense labeling in map\\nenhancement. We apply a methodology in which rooms are\\nlabeled strategically at decision points (e.g., near doors or\\nintersections). We later show that such label placement brings\\na considerable performance boost, which is particularly sig-\\nnificant in complex environments where navigation paths can\\ninvolve multiple rooms and transitions. This process can be\\nautomated (though not in this paper) to make it scalable for\\nbroader applications without manual intervention.\\nB. VLM-Based Plan Generation\\nIn our VLM-based framework, as shown in Fig. 2, VLMs\\nformulate navigation plans based on a floor plan image and\\na text prompt. This method leverages an instruction-based\\ntext prompting strategy, which involves providing the VLM\\nwith explicit and detailed guidelines for the navigation task,\\nalong with the floor plan image. We now elaborate on the\\nprompting strategy and the resulting output format.\\nThe text prompt given to the VLM is shown in Fig. 3.\\nThe prompt explicitly defines the starting point and the\\ndestination. This allows the VLM to understand the required\\nnavigation path and objectives clearly. It provides detailed\\ninstructions on interpreting the floor plan and managing door\\ninteractions. These guidelines include specific protocols for\\ndoor operations and decision-making processes.\\nA key aspect of this prompt is the request for all door\\nand room connections. By generating this information at the\\nstart, we speculate that the VLM integrates it with the floor\\nplan image to produce an accurate navigation sequence. We\\nbelieve this step helps the VLM better understand the spatial\\nrelationships in the map, leading to accurate navigation path\\nplanning.\\nBased on the text prompt and floor plan image, the VLM\\ngenerates a sequence of actions required to navigate from\\nthe initial to goal location. This sequence includes specific\\nsteps, e.g., approaching, opening, and passing through doors.\\nThe output from the VLM is a detailed sequence of actions\\nformatted as follows:\\n•ApproachDoor(x): Move in front of door x.\\n•OpenDoor(x): Open door x.\\nI am a robot that cannot go through walls and must use doors to navigate. This \\nis the floor plan of the building I am in right now (provided as an image). \\nYou are a navigation agent, and your task is to give me a detailed, efficient \\nnavigation plan that strictly follows a sequence of actions to achieve the \\nnavigation task: Begin in <location A>  and arrive at <location B> . The only \\ndoors which exist are represented as yellow rectangles and labeled with  'D(N)' \\ndistinct positive integers(1,2,3...N). A plan consists of a sequence of the \\nfollowing actions: \\n \\nApproachDoor(x) : Move in front of door x. \\nOpenDoor(x) : Open door x. \\nGoThrough(x) : Move through open door x to the location on the other side. \\nInclude only the necessary doors that are part of the path being used, and do \\nnot mention doors that won't be traversed even if they are in the path. \\nExplicit Room and Door Descriptions:  Alongside the image, make a clear \\nlist of all rooms and doors with their connections - which is to be used for the \\nnavigation task. \\nRemember that the door symbol can overlap with the boundaries or common \\nspaces. Remember to only use the generated door room connections for \\nmaking the action plan.  Double-check if each action is necessary and correct \\nfor traversal to the end goal. Common spaces (eg Hall) and larger rooms may \\nhave multiple instances of the same labels to help you understand their \\nboundaries. \\nImportant:  The doors close after every GoThrough(x)  action. Carefully \\ninspect the floor plan image to ensure the correct correspondence between \\ndoors and rooms. Prioritize providing a correct path over the shortest path. \\nMake sure the path avoids any unnecessary doors or rooms. If any \\nunnecessary doors or rooms are included, silently correct the plan before \\nproviding the final sequence. Give the final path in a json format. \\nRemember to make explicit connections for each door, then make a step by \\nstep solution for each navigation step and ONLY use the door-room \\nconnections to generate the final navigation plan. Fig. 3: Text prompt input to VLM to generate navigation\\nplans. We define the starting and ending locations, action\\ntypes, and ask for explicit room and door connections to\\ngain insights as to how the VLM understands the map.\\n•GoThrough(x): Move through open door xto the\\nlocation on the other side.\\nThe final navigation plan is output in JSON format, spec-\\nifying each action. This structured format facilitates easy\\ninterpretation and execution of the navigation instructions.\\nThis plan is then parsed and executed by the robot.\\nIV. E XPERIMENTS\\nIn order to evaluate whether the VLM produces accurate\\nnavigation plans, we design and run experiments over a\\ndataset of floor plans. The experiments are designed to\\nmeasure the effect of floor plan size, task difficulty, and label\\ndensity on the VLM’s plan accuracy.\\nA. Experimental Setup\\nOur study uses floor plan images from a publicly available\\ndataset CVC-FP [32], which includes detailed architectural\\nfloor plans for various building types. Three floor plans were\\nrandomly selected from those that contain 9-11 rooms and\\nclear labels. Those maps are referred to as “original maps”\\nand are shown in Fig. 4. Our experiments use two state-of-\\nthe-art VLMs: GPT-4o [33] and Claude-3.5 Sonnet [34].\\nFor each floor plan, we generate five pairs of start and\\ngoal locations. To account for the stochastic nature of VLM\\nresponses, we run each VLM ten times per navigation task,\"),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 3}, page_content='Fig. 4: Three maps used in our experiments for evaluating the performance of VLMs in map parsing and plan generation.\\nFig. 5: Example of a doubled map.\\nresulting in a total of 50 navigation trials per floor plan.\\nWe evaluate performance based on the correctness of the\\ngenerated plans. A plan is considered correct if it uses only\\nthose actions defined in the text prompt, includes feasible\\nactions, and leads a sequence of transitions from the start\\nlocation to the goal. Example infeasible actions include\\nnavigating to a room that is not connected to the current\\nroom and opening a door that belongs to a distant room.\\nOur experimental design focuses on three key dimensions:\\n1)Map Size: We hypothesize that increasing the map size\\nwill result in a decrease in VLM’s map parsing per-\\nformance. This hypothesis is based on the assumption\\nthat larger maps introduce greater complexity.\\n2)Task Difficulty: We hypothesize that when the start\\nand end locations are far away (based on number of\\nrooms required to traverse), the VLMs will have a low\\naccuracy in map parsing and plan generation.\\n3)Label Density: We hypothesize that a densely labelled\\nfloor plan map will facilitate accurate navigation plan\\ngeneration from VLMs.\\nNext, we describe our experiment setup for evaluating eachof the three hypotheses.\\n1) Map Size: To examine the impact of map size on\\nnavigation performance, we developed two types of maps:\\nOriginal Maps that are enhanced versions of the maps\\nselected from the CVC-FP dataset, and Doubled Maps that\\nwere created by connecting two copies of an original map\\nthrough an additional door. Fig. 5 presents an example of a\\ndoubled map. A door denoted as D10 is added to establish\\nconnectivity, thereby forming the final doubled map.\\n2) Task Difficulty: We design two types of navigation\\ntasks to evaluate model performance across two levels of\\ndifficulty. Easy Tasks are the navigation tasks that can be\\ncompleted by navigating from Room A to Room B without\\ntraversing any intermediate rooms. Hard Tasks are those that\\nrequire a robot to navigate through at least two intermediate\\nrooms before the goal can be achieved. As a result, an\\noptimal solution of hard tasks will involve four rooms in\\ntotal. The increased complexity introduces more decision\\npoints and possible paths, producing a more challenging task.\\n3) Label Density: We evaluated the impact of label den-\\nsity on model performance by implementing two labeling\\nschemes. Sparse-Labeled maps are those where each room\\nwas labeled with a single label, usually placed at the center.\\nThis minimalistic approach offered fewer cues for the model\\nto base its navigation decisions on. Dense-Labeled maps\\ninclude multiple labels for each room, where the placement\\nis described in Section III-A.\\nB. Experimental Results\\n1) Hypothesis 1 (Map Size): The first hypothesis explores\\nhow the size of the map influences the model’s accuracy.\\nWe compared the performance between original maps and\\ndoubled maps over hard tasks to assess the same.\\nThe results indicate that accuracy decreases as map size\\nincreases in the overall domain analysis, with the VLMs per-\\nforming better in smaller maps. The difference is significant,\\nas a T-test on trials with GPT-4o revealed a drop in accuracy\\n(t= 6.13,p < 0.0001 ). This supports our hypothesis that'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 4}, page_content='Domain 1 Domain 2 Domain 30.00.20.40.60.81.0 AccuracyOriginal Map\\nDoubled Map(a) Map Size\\nDomain 1 Domain 2 Domain 30.00.20.40.60.81.0\\nEasy Task\\nHard Task (b) Task Difficulty\\nDomain 1 Domain 2 Domain 30.00.20.40.60.81.0 Dense Labels\\nSparse Labels (c) Label Density\\nFig. 6: Comparison of GPT-4o results for map size, task difficulty and label density. We used original maps in the “label\\ndensity” experiment. To accommodate hard tasks, we used doubled maps in the “task difficulty” experiment.\\nDomain 1 Domain 2 Domain 30.00.20.40.60.81.0 AccuracyOriginal Map\\nDoubled Map\\n(a) Map Size\\nDomain 1 Domain 2 Domain 30.00.20.40.60.81.0\\nEasy Task\\nHard Task (b) Task Difficulty\\nDomain 1 Domain 2 Domain 30.00.20.40.60.81.0 Dense Labels\\nSparse Labels (c) Label Density\\nFig. 7: Comparison of Claude Sonnet 3.5 results for map size, task difficulty and label density. We used original maps in\\nthe “label density” experiment. To accommodate hard tasks, we used doubled maps in the “task difficulty” experiment.\\naccuracy decreases as map size increases. The results are\\nreported in Fig. 6 and 7. Both GPT and Claude exhibit a\\nsimilar pattern of performance decline with larger maps.\\n2) Hypothesis 2 (Task Difficulty): The second hypothesis\\ninvestigates how task difficulty impacts accuracy. Easy tasks\\ninvolve straightforward navigation between rooms, while\\nhard tasks, require traversing multiple intermediate rooms,\\nmaking the tasks more complex. We compared the perfor-\\nmance between doubled maps over easy tasks and hard tasks.\\nThe accuracy of the GPT-4o models generally decreased\\nwith increased task difficulty, as more complex tasks led to\\nlower accuracy in two out of the three maps. For example,\\na T-test on doubled map2indicated a drop in accuracy\\nwith t= 2.88andp= 0.0047 . The results from both\\nVLMs generally support our hypothesis that more complex\\ntasks lead to lower accuracy, as navigating through multiple\\nrooms introduces additional challenges. One example task is\\ndetailed in Table I.\\n3) Hypothesis 3 (Label Density): The third hypothesis\\nexamines the impact of label density on accuracy. This is\\nevaluated by comparing the performance of original mapsTABLE I: An example navigation plan generated by GPT-4o\\nin Map 1 shown on the left of Fig. 4. The initial location is\\n“Terrrasse Couverte” and the goal location is “Chambre 1”.\\nThis navigation task requires a sequence of nine actions.\\nGPT-4o achieved 0.96 success rate in this map on similar\\nhard navigation tasks, which demonstrates great promises for\\nVLM-based map parsing research.\\nNumber Action\\n1 ApproachDoor(D8)\\n2 OpenDoor(D8)\\n3 GoThrough(D8)\\n4 ApproachDoor(D7)\\n5 OpenDoor(D7)\\n6 GoThrough(D7)\\n7 ApproachDoor(D4)\\n8 OpenDoor(D4)\\n9 GoThrough(D4)\\nfrom sparse-label and dense-label datasets over hard tasks.\\nDense labels provide more contextual information, while\\nsparse labels offer minimal context, making the navigation'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 5}, page_content='Edit Map \\nLocalize \\nVLM\\n. \\n. \\n. . \\n. \\n. \\n. \\n. \\n. Edit Map \\nLocalize \\nApproachDoor(D1) \\nOpenDoor(D1) \\nGoThrough(D1) \\nApproachDoor(D2) \\n. \\n. \\n. \\nFig. 8: The robot localizes directly on the floor plan image and performs a navigation task. The occupancy grid map that\\nthe robot used for localization and navigation was also derived from the floor plan map captured by the robot. We manually\\nremoved the labels that would otherwise be interpreted by the robot as obstacles. From right to left, the robot was performing\\n1) Going through Door N09, 2) Approaching Door N00, 3) Opening Door N00, and 4) Approaching Door T01.\\ntask more challenging.\\nThe accuracy of the GPT-4o models significantly improved\\nwith dense labels compared to sparse labels across all\\nmaps. For instance, a T-test on original map1showed\\na significant improvement with t= 10.72andp < 0.0001 .\\nThis result indicates that dense labels substantially enhance\\naccuracy, supporting our hypothesis that dense labels provide\\ncrucial contextual information, enabling the models to nav-\\nigate more effectively. The improvement in accuracy with\\ndense labels was consistent across all maps for GPT-4o,\\nhighlighting the importance of label density in successful\\nnavigation.\\nThese findings suggest that our approach can effectively\\nhandle a range of navigation challenges while demonstrating\\nthe critical importance of label density, task difficulty, and\\nmap size in determining overall performance.\\nV. H ARDWARE DEMONSTRATION\\nOur VLM-based planning and navigation system is\\ndemonstrated on a DEEPRobotics Lite3 quadruped robot. An\\nimage of the floor plan of a building on a college campus\\nis captured from the robot’s camera. This image is then\\nedited as described in Section III to make it suitable for the\\nVLM query. Another version of the raw image with space\\nwhited out is directly used for robot localization, as seen\\nin Fig. 8. The VLM is then queried with the edited photo,\\nand a navigation plan consisting of a sequence of navigation\\nand door opening actions is generated. The robot executes\\nthis navigation plan, to move from the robotics lab (room\\nN09), to a classroom (room T01). The robot localizes itself\\ndirectly on a grayscale version of the floor plan, without the\\nneed for generating an accurate occupancy grid via SLAM.\\nThe robot successfully avoids the obstacles not present onthe floor plan, asks for doors to be opened as needed, and\\nachieves the desired navigation goal.\\nVI. C ONCLUSION AND FUTURE WORK\\nIn this work, we introduce a novel task, named map\\nparsing, that is unexplored in the VLM literature while point-\\ning to the foundation of mobile robotics. We demonstrate\\nremarkable performance of two VLMs on map parsing tasks,\\nas applied to robot navigation. We develop a VLM-based\\nplanning system that generates navigation plans directly from\\na floor plan image and validated our approach through exper-\\niments on a floor plan dataset and on hardware, demonstrat-\\ning the feasibility of VLM-driven navigation across different\\nenvironments.\\nWhile our results show that this approach is viable, several\\nchallenges remain, opening up avenues for further research.\\nThe process of modifying floor plans to optimize VLM\\nperformance is still a manual task. Future research could in-\\nvestigate leveraging segmentation models [35], [36] or other\\ntechniques to automate map refinement. Also, we can investi-\\ngate strategies to improve the VLM’s ability to handle larger\\nand more complex maps, e.g., outdoor environments [37].\\nAnother direction is that VLMs (and all transformer based\\nautoregressive models) have a host of known issues, such\\nas hallucinations and biases [38], [39], [40], [41] that can\\nbe addressed in robot planning. We anticipate that this work\\nwill inspire further studies that expand upon the ideas on\\nVLM-based map parsing in this paper.\\nREFERENCES\\n[1] A. Elfes, “Using occupancy grids for mobile robot perception and\\nnavigation,” Computer , vol. 22, no. 6, pp. 46–57, 1989.\\n[2] T. Taketomi, H. Uchiyama, and S. Ikeda, “Visual slam algorithms: A\\nsurvey from 2010 to 2016,” IPSJ transactions on computer vision and\\napplications , vol. 9, pp. 1–11, 2017.'),\n",
       " Document(metadata={'source': 'data\\\\Vision Language Models.pdf', 'page': 6}, page_content='[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and\\nD. Parikh, “Vqa: Visual question answering,” in Proceedings of the\\nIEEE international conference on computer vision , 2015, pp. 2425–\\n2433.\\n[4] Y .-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y . Cheng,\\nand J. Liu, “Uniter: Universal image-text representation learning,” in\\nEuropean conference on computer vision . Springer, 2020, pp. 104–\\n120.\\n[5] T. Brooks, A. Holynski, and A. A. Efros, “Instructpix2pix: Learning\\nto follow image editing instructions,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2023, pp.\\n18 392–18 402.\\n[6] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta,\\nA. Xie, D. Driess, A. Wahid, Z. Xu, et al. , “Pivot: Iterative visual\\nprompting elicits actionable knowledge for vlms,” arXiv preprint\\narXiv:2402.07872 , 2024.\\n[7] F. Liu, K. Fang, P. Abbeel, and S. Levine, “Moka: Open-vocabulary\\nrobotic manipulation through mark-based visual prompting,” arXiv\\npreprint arXiv:2403.03174 , 2024.\\n[8] S. Zhang, F. Yang, P. Khandelwal, and P. Stone, “Mobile robot\\nplanning using action language with an abstraction hierarchy,” in\\nInternational Conference on Logic Programming and Nonmonotonic\\nReasoning . Springer, 2015, pp. 502–516.\\n[9] Y . Hayamizu, S. Amiri, K. Chandan, K. Takadama, and S. Zhang,\\n“Guiding robot exploration in reinforcement learning via automated\\nplanning,” in Proceedings of the International Conference on Auto-\\nmated Planning and Scheduling , vol. 31, 2021, pp. 625–633.\\n[10] Z. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak,\\n“Coupling vision and proprioception for navigation of legged robots,”\\ninProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , 2022, pp. 17 273–17 283.\\n[11] D. DeFazio, E. Hirota, and S. Zhang, “Seeing-eye quadruped navi-\\ngation with force responsive locomotion control,” in Conference on\\nRobot Learning . PMLR, 2023, pp. 2184–2194.\\n[12] C. Zhang, Z. Yang, Q. Fang, C. Xu, H. Xu, X. Xu, and J. Zhang,\\n“Frl-slam: A fast, robust and lightweight slam system for quadruped\\nrobot navigation,” in 2021 IEEE International Conference on Robotics\\nand Biomimetics (ROBIO) . IEEE, 2021, pp. 1165–1170.\\n[13] A. Macario Barros, M. Michel, Y . Moline, G. Corre, and F. Carrel, “A\\ncomprehensive survey of visual slam algorithms,” Robotics , vol. 11,\\nno. 1, p. 24, 2022.\\n[14] M. Sorokin, J. Tan, C. K. Liu, and S. Ha, “Learning to navigate\\nsidewalks in outdoor environments,” IEEE Robotics and Automation\\nLetters , vol. 7, no. 2, pp. 3906–3913, 2022.\\n[15] H. Hwang, T. Xia, I. Keita, K. Suzuki, J. Biswas, S. I. Lee, and\\nD. Kim, “System configuration and navigation of a guide dog robot:\\nToward animal guide dog-level guiding work,” in IEEE International\\nConference on Robotics and Automation (ICRA) , 2023.\\n[16] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V .\\nLe, D. Zhou, et al. , “Chain-of-thought prompting elicits reasoning in\\nlarge language models,” Advances in neural information processing\\nsystems , vol. 35, pp. 24 824–24 837, 2022.\\n[17] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, “A survey on in-context learning,” arXiv preprint\\narXiv:2301.00234 , 2022.\\n[18] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao, “Set-of-mark\\nprompting unleashes extraordinary visual grounding in gpt-4v,” arXiv\\npreprint arXiv:2310.11441 , 2023.\\n[19] Y . Kant, A. Ramachandran, S. Yenamandra, I. Gilitschenski, D. Batra,\\nA. Szot, and H. Agrawal, “Housekeep: Tidying virtual households\\nusing commonsense reasoning,” in European Conference on Computer\\nVision . Springer, 2022, pp. 355–373.\\n[20] Y . Ding, X. Zhang, C. Paxton, and S. Zhang, “Task and motion\\nplanning with large language models for object rearrangement,” in\\n2023 IEEE/RSJ International Conference on Intelligent Robots and\\nSystems (IROS) . IEEE, 2023, pp. 2086–2092.\\n[21] N. Yokoyama, S. Ha, D. Batra, J. Wang, and B. Bucher, “Vlfm: Vision-\\nlanguage frontier maps for zero-shot semantic navigation,” in 2024\\nIEEE International Conference on Robotics and Automation (ICRA) .\\nIEEE, 2024, pp. 42–48.\\n[22] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, and A. Velasquez,\\n“Saynav: Grounding large language models for dynamic planning to\\nnavigation in new environments,” in Proceedings of the International\\nConference on Automated Planning and Scheduling , vol. 34, 2024, pp.\\n464–474.[23] D. Song, J. Liang, A. Payandeh, X. Xiao, and D. Manocha, “Socially\\naware robot navigation through scoring using vision-language models,”\\narXiv preprint arXiv:2404.00210 , 2024.\\n[24] A. J. Sathyamoorthy, K. Weerakoon, M. Elnoor, A. Zore, B. Ichter,\\nF. Xia, J. Tan, W. Yu, and D. Manocha, “Convoi: Context-aware\\nnavigation using vision language models in outdoor and indoor en-\\nvironments,” arXiv preprint arXiv:2403.15637 , 2024.\\n[25] A. S. Chen, A. M. Lessing, A. Tang, G. Chada, L. Smith, S. Levine,\\nand C. Finn, “Commonsense reasoning for legged robot adaptation\\nwith vision-language models,” arXiv preprint arXiv:2407.02666 , 2024.\\n[26] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-\\nT. L. Chiang, T. Erez, L. Hasenclever, J. Humplik, et al. , “Language to\\nrewards for robotic skill synthesis,” arXiv preprint arXiv:2306.08647 ,\\n2023.\\n[27] Y . Tang, W. Yu, J. Tan, H. Zen, A. Faust, and T. Harada, “Saytap: Lan-\\nguage to quadrupedal locomotion,” arXiv preprint arXiv:2306.07580 ,\\n2023.\\n[28] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. , “Do as i\\ncan, not as i say: Grounding language in robotic affordances,” arXiv\\npreprint arXiv:2204.01691 , 2022.\\n[29] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar, et al. , “Inner monologue:\\nEmbodied reasoning through planning with language models,” arXiv\\npreprint arXiv:2207.05608 , 2022.\\n[30] B. Liu, Y . Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone,\\n“Llm+ p: Empowering large language models with optimal planning\\nproficiency,” arXiv preprint arXiv:2304.11477 , 2023.\\n[31] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu,\\nL. Takayama, F. Xia, J. Varley, et al. , “Robots that ask for help:\\nUncertainty alignment for large language model planners,” arXiv\\npreprint arXiv:2307.01928 , 2023.\\n[32] L.-P. de las Heras, O. Terrades, S. Robles, and G. S’anchez, “Cvc-\\nfp and sgt: a new database for structural floor plan analysis and its\\ngroundtruthing tool,” International Journal on Document Analysis and\\nRecognition , 2015.\\n[33] OpenAI, “Gpt-4o,” https://openai.com/index/hello-gpt-4o/.\\n[34] Anthropic, “Claude-3.5 sonnet,” https://www.anthropic.com/news/\\nclaude-3-5-sonnet.\\n[35] C. Zhang, D. Han, Y . Qiao, J. U. Kim, S.-H. Bae, S. Lee, and C. S.\\nHong, “Faster segment anything: Towards lightweight sam for mobile\\napplications,” arXiv preprint arXiv:2306.14289 , 2023.\\n[36] N. Ravi, V . Gabeur, Y .-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr,\\nR. R ¨adle, C. Rolland, L. Gustafson, et al. , “Sam 2: Segment anything\\nin images and videos,” arXiv preprint arXiv:2408.00714 , 2024.\\n[37] D. Shah, B. Osi ´nski, b. ichter, and S. Levine, “Lm-nav: Robotic\\nnavigation with large pre-trained models of language, vision, and\\naction,” in Proceedings of The 6th Conference on Robot Learning ,\\nser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and\\nJ. Ichnowski, Eds., vol. 205. PMLR, 14–18 Dec 2023, pp. 492–504.\\n[Online]. Available: https://proceedings.mlr.press/v205/shah23b.html\\n[38] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang,\\nE. Zhao, Y . Zhang, Y . Chen, L. Wang, A. T. Luu, W. Bi,\\nF. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on\\nhallucination in large language models,” 2023. [Online]. Available:\\nhttps://arxiv.org/abs/2309.01219\\n[39] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,\\n“On the dangers of stochastic parrots: Can language models be too\\nbig?” in Proceedings of the 2021 ACM Conference on Fairness,\\nAccountability, and Transparency , ser. FAccT ’21. New York, NY ,\\nUSA: Association for Computing Machinery, 2021, p. 610–623.\\n[Online]. Available: https://doi.org/10.1145/3442188.3445922\\n[40] W. M. Si, M. Backes, J. Blackburn, E. De Cristofaro, G. Stringhini,\\nS. Zannettou, and Y . Zhang, “Why So Toxic? Measuring and Trigger-\\ning Toxic Behavior in Open-Domain Chatbots,” in Proceedings of the\\n2022 ACM SIGSAC Conference on Computer and Communications\\nSecurity , ser. CCS ’22, 2022, pp. 2659–2673.\\n[41] S. Garg and G. Ramakrishnan, “BAE: BERT-based adversarial\\nexamples for text classification,” in Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing\\n(EMNLP) , B. Webber, T. Cohn, Y . He, and Y . Liu, Eds. Online:\\nAssociation for Computational Linguistics, Nov. 2020, pp. 6174–6181.\\n[Online]. Available: https://aclanthology.org/2020.emnlp-main.498')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process the pdf documents\n",
    "loader = PyPDFDirectoryLoader(\"data/\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split documents\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.014088926836848259, -0.039512742310762405, -0.015114354901015759, 0.0066186427138745785, -0.0415877141058445, 0.002742007840424776, 0.06357996165752411, -0.0138254938647151, -0.027112694457173347, -0.018800072371959686, 0.10804460942745209, -0.0650469958782196, 0.021947627887129784, -0.0778966173529625, -0.07859108597040176, 0.030012499541044235, 0.054338812828063965, 0.001650859834626317, -0.0864286720752716, -0.052418094128370285, -0.06948328018188477, -4.420383902470348e-06, 0.009225304238498211, 0.01851612702012062, 0.03880367428064346, 0.05019953474402428, 0.056104302406311035, 0.02819352224469185, 0.015586882829666138, -0.06893792003393173, -0.009146508760750294, 0.014944170601665974, 0.0717703104019165, 0.05648346617817879, -0.018978869542479515, 0.008881008252501488, 0.01944255642592907, 0.052173301577568054, -0.052420616149902344, 0.020819779485464096, -0.004502603318542242, -0.019129369407892227, 0.03650399297475815, 0.024134639650583267, 0.0074194008484482765, 0.02198060043156147, -0.003312151413410902, -0.009592625312507153, 0.06063034012913704, 0.051031943410634995, -0.06557834893465042, 0.003926756791770458, -0.02638270892202854, 0.08044970035552979, 0.00027804309502243996, -0.05347934365272522, -0.01916728913784027, 0.03238416835665703, 0.007533031515777111, -0.017865853384137154, 0.021303541958332062, -0.07853072136640549, -0.05412476882338524, 0.03674883022904396, 0.044115085154771805, 0.015321295708417892, -0.022556263953447342, 0.04140842333436012, 0.01010207924991846, -0.13332709670066833, -0.05688288435339928, 0.036225881427526474, 0.00017749906692188233, 0.05292610824108124, 0.005417363252490759, 0.0027450721245259047, 0.0077401879243552685, -0.037449173629283905, 0.10559894144535065, -0.08585123717784882, -0.03775588050484657, -0.030930105596780777, -0.015393257141113281, 0.08726045489311218, 0.05539306625723839, -0.08058369159698486, 0.007036382798105478, 0.08694561570882797, -0.0652056336402893, -0.0007866898085922003, -0.04500306397676468, 0.04935259744524956, 0.08309466391801834, 0.013997530564665794, -0.10451265424489975, 0.07662838697433472, 0.04236094653606415, -0.047443196177482605, 0.0653751790523529, 0.1296536773443222, -0.03848344832658768, 0.05598744750022888, -0.01042377669364214, 0.06998690962791443, -0.041900914162397385, -0.09764057397842407, -7.450234261341393e-05, -0.021162578836083412, 0.037217941135168076, -0.052095893770456314, -0.0048361183144152164, -0.02716769278049469, -0.09962300956249237, -0.09691863507032394, 0.052376601845026016, -0.08187975734472275, -0.09879544377326965, 0.029900260269641876, -0.08992508053779602, 0.014723586849868298, -0.07040566205978394, -0.008330412209033966, 0.015459110960364342, -0.03204178065061569, 0.01013887207955122, -0.045422960072755814, -0.06796110421419144, -8.919056958379375e-34, 0.06152648478746414, -0.08226466178894043, 0.01420217752456665, -0.03180520609021187, 0.003798706689849496, -0.07469727843999863, -0.023348743095993996, -0.0573490746319294, -0.03136557713150978, 0.023167619481682777, -0.07971657812595367, 0.025133546441793442, -0.0524856261909008, 0.04914061725139618, 0.06845898926258087, 0.01481988001614809, 0.014099564403295517, 0.0465746633708477, -0.03297904133796692, 0.026480844244360924, 0.027733556926250458, 0.02079380303621292, 0.07383082807064056, -0.031770430505275726, 0.03213370963931084, -0.011553214862942696, -0.02465849556028843, 0.012796358205378056, 0.0046825348399579525, 0.04108481481671333, -0.030686650425195694, 0.06792552024126053, -0.011902440339326859, 0.03108433447778225, 0.018210837617516518, -0.042371898889541626, 0.014818304218351841, -0.03025667928159237, 0.12138964235782623, 0.0027351966127753258, 0.08100497722625732, 3.1233899790095165e-05, 0.066978819668293, 0.014359737746417522, -0.03459072485566139, 0.03366775065660477, 0.08024662733078003, 0.07357019186019897, 0.038478344678878784, 0.06602954864501953, -0.018090663477778435, -0.028791997581720352, -0.04834160581231117, 0.0059625618159770966, -0.01809573918581009, 0.06854438781738281, -0.027376845479011536, -0.028067544102668762, 0.04018055275082588, 0.036696936935186386, -0.05139586701989174, 0.07448115199804306, 0.03650432452559471, 0.037297483533620834, -0.051520150154829025, 0.002275594510138035, -0.02261141687631607, -0.06269703805446625, 0.05736054852604866, 0.021281985566020012, -0.0009131473489105701, -0.007948717102408409, 0.03041418455541134, 0.050441570580005646, 0.044328104704618454, 0.03548815846443176, -0.01102334726601839, 0.05829205736517906, -0.16823002696037292, 0.04848332330584526, -0.03290994092822075, -0.038055937737226486, -0.04930269718170166, -0.006703845225274563, -0.020292775705456734, 0.05905330926179886, 0.015721485018730164, -0.047976214438676834, 0.003968642093241215, -0.01150085311383009, -0.1465466469526291, 0.02853410318493843, -0.013951042667031288, -0.03184802085161209, -0.03898613899946213, 1.10951008765529e-33, -0.032266102731227875, 0.05223323404788971, -0.04015771299600601, 0.06915269792079926, 0.04456872120499611, 0.028362581506371498, -0.013043690472841263, 0.016189318150281906, 0.037665579468011856, 0.10575994849205017, 0.049403078854084015, -0.04649679735302925, 0.045069947838783264, -0.08968806266784668, -0.01633829064667225, 0.05636952072381973, -0.0006496328278444707, -0.028422102332115173, -0.04208868369460106, -0.0427427664399147, -0.08151879906654358, -0.029408149421215057, -0.09202492982149124, 0.0019680934492498636, -0.03395991027355194, 0.00363605422899127, -0.0392131470143795, 0.010545426048338413, -0.03479811176657677, -0.012585727497935295, -0.03113851509988308, -0.09207086265087128, -0.04506109282374382, -0.028842296451330185, -0.10618359595537186, 0.018926795572042465, 0.12201357632875443, -0.04031510278582573, -0.032918743789196014, 0.03030259720981121, 0.05426932871341705, 0.05824197828769684, -0.08905373513698578, 0.13108794391155243, -0.024133678525686264, -0.055914994329214096, -0.052850060164928436, 0.08143237233161926, 0.04772986099123955, 0.00029619026463478804, -0.043808624148368835, 0.02110821194946766, 0.048310890793800354, -0.0029504282865673304, 0.07366311550140381, -0.06449198722839355, 0.052854616194963455, 0.027268698439002037, -0.020313946530222893, 0.04076479375362396, -0.044460229575634, -0.04363140091300011, -0.01934044249355793, 0.1267063021659851, -0.0019944971427321434, -0.013256202451884747, -0.04319757595658302, -0.0669948011636734, -0.06632301211357117, 0.025130875408649445, 0.055048756301403046, -0.01837845891714096, 0.039852045476436615, -0.0297908503562212, -0.07739710062742233, -0.028974900022149086, -0.0988713800907135, -0.0013606729917228222, 0.034912824630737305, 0.06892453134059906, 0.06183405965566635, -0.07104526460170746, 0.03815903142094612, 0.10541268438100815, 0.01911037229001522, 0.08708393573760986, 0.07154734432697296, -0.0414729081094265, -0.022439995780587196, 0.014021534472703934, -0.04737025871872902, -0.03791164979338646, -0.06839173287153244, 0.08491494506597519, -0.012318669818341732, -1.251009162928085e-08, -0.00633802218362689, -0.02178131975233555, 0.07300151139497757, -0.014383218251168728, 0.008236038498580456, 0.004037126433104277, -0.009705218486487865, 0.14998576045036316, -0.0032524731941521168, 0.05324191600084305, 0.0964404046535492, -0.004490958992391825, -0.027159491553902626, 0.015615974552929401, -0.037892021238803864, -0.03219812735915184, 0.06705980002880096, 0.03883844614028931, -0.01384683232754469, 0.0844578742980957, 0.0605551116168499, 0.014965994283556938, 0.014951462857425213, -0.008289145305752754, 0.12431885302066803, -0.02149086259305477, -0.009541117586195469, 0.08909007906913757, -0.029276207089424133, 0.029995176941156387, -0.007451281417161226, -0.015707453712821007, 0.07408270984888077, -0.09494797885417938, 0.05654922500252724, -0.006976025644689798, -0.02612954005599022, -0.0013068875996395946, -0.06051428243517876, -0.05830710753798485, -0.0019605138804763556, 0.004393311217427254, -0.026797376573085785, -0.02124794013798237, 0.01269897073507309, 0.03206409513950348, 0.0710197314620018, -0.041789423674345016, 0.08379390090703964, 0.02153119631111622, -0.010252838023006916, 0.024844247847795486, 0.019313812255859375, 0.013504153117537498, 0.05382504686713219, 0.059634312987327576, 0.019622061401605606, -0.02478276565670967, 0.014937307685613632, 0.013963393867015839, -0.004557439126074314, -0.003747426439076662, -0.052841916680336, -0.012030234560370445]\n"
     ]
    }
   ],
   "source": [
    "#create embeddings\n",
    "def get_embedding_function():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"Dataset\")\n",
    "print(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3192872380421534}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator='embedding_distance',\n",
    "                        embeddings = embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction = \"King\", reference = \"Queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "#create Vector DB\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "    #create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "\n",
    "    #ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    uniques_chunks = []\n",
    "\n",
    "    for chunk, id in zip(chunks, ids):\n",
    "        if id not in unique_ids:\n",
    "            unique_ids.add(id)\n",
    "            uniques_chunks.append(chunk)\n",
    "    \n",
    "    #create a new Chroma DB from the Documents\n",
    "    vectorstore = Chroma.from_documents(documents = uniques_chunks,\n",
    "                                        ids= list(unique_ids),\n",
    "                                        embedding=embedding_function,\n",
    "                                        persist_directory=vectorstore_path)\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Vectorstore\n",
    "\n",
    "vectorstore = create_vectorstore(chunks = chunks,\n",
    "                                embedding_function=embedding_function,\n",
    "                                vectorstore_path=\"vectorstore_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore = Chroma(persist_directory=\"vectorstore_test\", embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a retriever \n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt template\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, say that you don't know. \n",
    "DON'T MAKE UP ANYTHING. DO NOT HALLUCINATE.\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "-----\n",
    "answer the question based on the above context: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate Structured responses\n",
    "\n",
    "class AnswerWithSources(BaseModel):\n",
    "    \"Answer to the question, with sources and reasoning.\"\n",
    "    answer: str = Field(description=\"Answer to the question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question.\")\n",
    "    reasoning: str = Field(description=\"explain the reasoning used to answer the question\")\n",
    "\n",
    "\n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"Extracted information from the context.\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources\n",
    "    publication_year: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "4 validation errors for ExtractedInfo\npaper_title\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='Llama 2: Open foundation...d chat models. Preprint', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npaper_summary\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value=\"The authors present Llam...e improved performance.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npaper_authors\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='Cynthia Gao, Vedanuj Gos...nov, and Thomas Scialom', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npublication_year\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='2023', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs)\n\u001b[0;32m      5\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m format_docs , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m|\u001b[39mprompt_template\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m|\u001b[39mllm\u001b[38;5;241m.\u001b[39mwith_structured_output(ExtractedInfo)\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me the title, summary, publication date, authors of the research paper.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:185\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    183\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    197\u001b[0m             config,\n\u001b[0;32m    198\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1923\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1919\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1920\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1921\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1922\u001b[0m         Output,\n\u001b[1;32m-> 1923\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1931\u001b[0m     )\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1933\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:186\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    183\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 186\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    190\u001b[0m             config,\n\u001b[0;32m    191\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    192\u001b[0m         )\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    197\u001b[0m             config,\n\u001b[0;32m    198\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:297\u001b[0m, in \u001b[0;36mPydanticToolsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_tool_only:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pydantic_objects[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m pydantic_objects \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:292\u001b[0m, in \u001b[0;36mPydanticToolsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool arguments must be specified as a dict, received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m         )\n\u001b[1;32m--> 292\u001b[0m     pydantic_objects\u001b[38;5;241m.\u001b[39mappend(\u001b[43mname_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "File \u001b[1;32mc:\\Users\\pkhobragade\\Desktop\\Langgraph\\structured_rag\\stru_rag\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 4 validation errors for ExtractedInfo\npaper_title\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='Llama 2: Open foundation...d chat models. Preprint', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npaper_summary\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value=\"The authors present Llam...e improved performance.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npaper_authors\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='Cynthia Gao, Vedanuj Gos...nov, and Thomas Scialom', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type\npublication_year\n  Input should be a valid dictionary or instance of AnswerWithSources [type=model_type, input_value='2023', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "#generate responses\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    |prompt_template\n",
    "    |llm.with_structured_output(ExtractedInfo)\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AIMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m reasoning_row \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 10\u001b[0m     answer_row\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m     source_row\u001b[38;5;241m.\u001b[39mappend(df[col][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m     reasoning_row\u001b[38;5;241m.\u001b[39mappend(df[col][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'AIMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stru_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
